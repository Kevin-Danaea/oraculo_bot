#!/usr/bin/env python3
"""
Procesador de datos hist√≥ricos de Reddit con an√°lisis de sentimiento.
Procesa archivos .zst masivos de Reddit y los enriquece con an√°lisis usando Gemini.

Uso:
    python procesador_historico.py path/to/reddit_data.zst

Caracter√≠sticas:
- Procesamiento en streaming (l√≠nea por l√≠nea) para eficiencia de memoria
- Inserci√≥n en lotes para eficiencia de base de datos
- Manejo robusto de rate limits de API con exponential backoff
- Barra de progreso visual con tqdm
"""

import argparse
import io
import json
import logging
import os
import subprocess
import sys
import time
import zstandard as zstd
from datetime import datetime
from typing import Dict, Any, List, Optional, Set
from urllib.parse import urlparse
import psutil

import pandas as pd
from google import genai
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker, Session
from tqdm import tqdm

# Importar configuraci√≥n y modelos del proyecto
from shared.config.settings import settings
from shared.database.models import Base, Noticia

# Configuraci√≥n del script
BATCH_SIZE = 500  # Tama√±o del lote para inserci√≥n en BD
MAX_RETRIES = 5   # M√°ximo n√∫mero de reintentos para API
INITIAL_RETRY_DELAY = 5  # Delay inicial en segundos para exponential backoff
DB_INSERT_RETRIES = 3  # Reintentos para inserci√≥n en base de datos
FALLBACK_DB_PATH = "fallback_oraculo.db"  # Ruta de la base de datos SQLite de fallback
CHECKPOINT_FILE = "procesamiento_checkpoint.txt"  # Archivo para guardar progreso
PROCESSED_URLS_FILE = "urls_procesadas.txt"  # Archivo para guardar URLs ya procesadas

# Listas de vocabulario controlado para validaci√≥n
VALID_EMOTIONS = ['Euforia', 'Optimismo', 'Neutral', 'Incertidumbre', 'Miedo']
VALID_CATEGORIES = ['Regulaci√≥n', 'Tecnolog√≠a/Adopci√≥n', 'Mercado/Trading', 'Seguridad', 'Macroeconom√≠a']

# Subreddits relevantes para crypto
CRYPTO_SUBREDDITS = {
    'CryptoCurrency', 'cryptocurrency', 'Bitcoin', 'bitcoin', 'ethereum', 'Ethereum',
    'ethtrader', 'defi', 'DeFi', 'CryptoMoonShots', 'cryptomoonshots', 'SatoshiStreetBets',
    'altcoin', 'CryptoCurrencyTrading', 'CryptoMarkets', 'Crypto_com', 'binance',
    'Coinbase', 'dogecoin', 'litecoin', 'ripple', 'cardano'
}

# Dominios de noticias confiables
TRUSTED_NEWS_DOMAINS = {
    'coindesk.com', 'cointelegraph.com', 'decrypt.co', 'bloomberg.com',
    'reuters.com', 'cnbc.com', 'forbes.com', 'wsj.com', 'ft.com',
    'coinbase.com', 'kraken.com', 'binance.com', 'crypto.com'
}

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('procesador_historico.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# Configurar el handler del stdout para UTF-8 en Windows
if sys.platform == "win32":
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')

def check_database_connection() -> bool:
    """
    Verifica que la conexi√≥n a la base de datos de Neon funcione correctamente.
    
    Returns:
        True si la conexi√≥n es exitosa, False en caso contrario
    """
    logger.info("[CHECK] Verificando conexi√≥n a la base de datos...")
    try:
        engine = create_engine(settings.DATABASE_URL)
        with engine.connect() as connection:
            # Ejecutar una consulta simple para verificar la conexi√≥n
            result = connection.execute(text("SELECT 1 as test_connection"))
            row = result.fetchone()
            
            if row and row[0] == 1:
                logger.info("[OK] Conexi√≥n a la base de datos verificada exitosamente")
                return True
            else:
                logger.error("[ERROR] La consulta de prueba no devolvi√≥ el resultado esperado")
                return False
                
    except Exception as e:
        logger.error(f"[ERROR] Error al conectar con la base de datos: {e}")
        return False

def check_gemini_api() -> bool:
    """
    Verifica que la API de Gemini funcione correctamente con una llamada de prueba.
    
    Returns:
        True si la API funciona correctamente, False en caso contrario
    """
    logger.info("[CHECK] Verificando conexi√≥n a la API de Gemini...")
    try:
        client = genai.Client(api_key=settings.GOOGLE_API_KEY)
        
        # Texto de prueba simple
        test_text = "Bitcoin alcanza nuevo m√°ximo hist√≥rico tras adopci√≥n institucional"
        test_prompt = f"""
        Analiza el siguiente texto sobre criptomonedas y devuelve √öNICAMENTE un JSON v√°lido con esta estructura:
        
        {{
            "sentiment_score": float entre -1.0 y 1.0,
            "primary_emotion": "una de estas opciones EXACTAS: {', '.join(VALID_EMOTIONS)}",
            "news_category": "una de estas opciones EXACTAS: {', '.join(VALID_CATEGORIES)}"
        }}
        
        Texto: "{test_text}"
        
        Responde √öNICAMENTE con el JSON:
        """
        
        response = client.models.generate_content(
            model='gemini-1.5-flash',
            contents=test_prompt
        )
        
        if not response.text or response.text.strip() == "":
            logger.error("[ERROR] La API de Gemini devolvi√≥ una respuesta vac√≠a")
            return False
        
        # Intentar parsear la respuesta como JSON
        try:
            clean_text = response.text.strip()
            if clean_text.startswith('```json'):
                clean_text = clean_text.replace('```json\n', '').replace('\n```', '')
            elif clean_text.startswith('```'):
                lines = clean_text.split('\n')
                clean_text = '\n'.join(lines[1:-1])
            
            result = json.loads(clean_text)
            
            # Validar que tiene las claves esperadas
            required_keys = ['sentiment_score', 'primary_emotion', 'news_category']
            for key in required_keys:
                if key not in result:
                    logger.error(f"[ERROR] La respuesta de Gemini no contiene la clave '{key}'")
                    return False
            
            # Validar tipos y valores
            sentiment_score = result.get('sentiment_score')
            if not isinstance(sentiment_score, (int, float)) or not (-1.0 <= sentiment_score <= 1.0):
                logger.error(f"[ERROR] sentiment_score inv√°lido: {sentiment_score}")
                return False
            
            primary_emotion = result.get('primary_emotion')
            if primary_emotion not in VALID_EMOTIONS:
                logger.error(f"[ERROR] primary_emotion inv√°lida: {primary_emotion}")
                return False
            
            news_category = result.get('news_category')
            if news_category not in VALID_CATEGORIES:
                logger.error(f"[ERROR] news_category inv√°lida: {news_category}")
                return False
            
            logger.info("[OK] API de Gemini verificada exitosamente")
            logger.info(f"   Respuesta de prueba: Score={sentiment_score:.2f}, Emoci√≥n={primary_emotion}, Categor√≠a={news_category}")
            return True
            
        except json.JSONDecodeError as e:
            logger.error(f"[ERROR] Error parseando JSON de respuesta de Gemini: {e}")
            logger.error(f"   Respuesta recibida: '{response.text[:200]}...'")
            return False
            
    except Exception as e:
        logger.error(f"[ERROR] Error al probar la API de Gemini: {e}")
        return False

def check_zstd_availability() -> bool:
    """
    Verifica que el comando zstd est√© disponible en el sistema.
    Proporciona instrucciones de instalaci√≥n si no est√° disponible.
    
    Returns:
        True si zstd est√° disponible, False en caso contrario
    """
    logger.info("[CHECK] Verificando disponibilidad del comando zstd...")
    try:
        result = subprocess.run(['zstd', '--version'], capture_output=True, check=True, text=True)
        version_info = result.stdout.strip().split('\n')[0] if result.stdout else "versi√≥n desconocida"
        logger.info(f"[OK] Comando zstd encontrado: {version_info}")
        return True
    except FileNotFoundError:
        logger.error("[ERROR] Comando 'zstd' no encontrado en el sistema")
        logger.error("")
        logger.error("üí° INSTRUCCIONES DE INSTALACI√ìN:")
        logger.error("   üì¶ Windows (con winget):")
        logger.error("       winget install facebook.zstd")
        logger.error("")
        logger.error("   üì¶ Windows (manual):")
        logger.error("       1. Descarga desde: https://github.com/facebook/zstd/releases")
        logger.error("       2. Extrae zstd.exe a una carpeta en tu PATH")
        logger.error("")
        logger.error("   üì¶ macOS:")
        logger.error("       brew install zstd")
        logger.error("")
        logger.error("   üì¶ Ubuntu/Debian:")
        logger.error("       sudo apt install zstd")
        logger.error("")
        return False
    except subprocess.CalledProcessError as e:
        logger.error(f"[ERROR] Error ejecutando comando zstd: {e}")
        return False

class SentimentAnalyzer:
    """Analizador de sentimientos usando Google Gemini."""
    
    def __init__(self):
        """Inicializa el cliente de Gemini."""
        try:
            self.client = genai.Client(api_key=settings.GOOGLE_API_KEY)
            logger.info("Cliente de Gemini inicializado correctamente")
        except Exception as e:
            logger.error(f"Error al configurar el cliente de Gemini: {e}")
            self.client = None
    
    def analyze_text_with_gemini(self, text: str) -> Dict[str, Any]:
        """
        Analiza el texto usando Gemini con manejo de rate limits.
        
        Args:
            text: Texto a analizar
            
        Returns:
            Dict con sentiment_score, primary_emotion y news_category
        """
        if not self.client:
            logger.warning("Cliente de Gemini no disponible, usando valores por defecto")
            return self._get_default_analysis()
        
        prompt = self._build_prompt(text)
        
        for attempt in range(MAX_RETRIES):
            try:
                response = self.client.models.generate_content(
                    model='gemini-1.5-flash',
                    contents=prompt
                )
                
                # Pausa breve para no saturar la API
                time.sleep(1)
                
                if not response.text or response.text.strip() == "":
                    logger.warning(f"Respuesta vac√≠a para: '{text[:50]}...'")
                    return self._get_default_analysis()
                
                return self._parse_response(response.text, text)
                
            except Exception as e:
                error_msg = str(e).lower()
                
                # Manejo espec√≠fico de rate limits
                if "resource_exhausted" in error_msg or "429" in error_msg:
                    delay = INITIAL_RETRY_DELAY * (2 ** attempt)
                    logger.warning(f"Rate limit alcanzado. Esperando {delay}s (intento {attempt + 1}/{MAX_RETRIES})")
                    time.sleep(delay)
                    continue
                else:
                    logger.error(f"Error no recuperable analizando texto: {e}")
                    return self._get_default_analysis()
        
        logger.error(f"Se agotaron los reintentos para: '{text[:50]}...'")
        return self._get_default_analysis()
    
    def _build_prompt(self, text: str) -> str:
        """Construye el prompt para Gemini."""
        return f"""
        Analiza el siguiente texto sobre criptomonedas y devuelve √öNICAMENTE un JSON v√°lido con esta estructura:
        
        {{
            "sentiment_score": float entre -1.0 y 1.0,
            "primary_emotion": "una de estas opciones EXACTAS: {', '.join(VALID_EMOTIONS)}",
            "news_category": "una de estas opciones EXACTAS: {', '.join(VALID_CATEGORIES)}"
        }}
        
        CRITERIOS PARA SENTIMENT_SCORE:
        - Muy positivo (adopci√≥n masiva, ATH, buenas regulaciones): 0.6 a 1.0
        - Positivo (desarrollos, adopci√≥n gradual): 0.1 a 0.5
        - Neutral (informativo, sin impacto claro): -0.1 a 0.1
        - Negativo (regulaciones adversas, ca√≠das): -0.5 a -0.1
        - Muy negativo (hacks, prohibiciones, crisis): -1.0 a -0.6
        
        CRITERIOS PARA PRIMARY_EMOTION:
        - Euforia: ATH, adopci√≥n masiva, noticias revolucionarias
        - Optimismo: Desarrollos positivos, buenas noticias graduales
        - Neutral: Noticias informativas sin carga emocional
        - Incertidumbre: Rumores, decisiones pendientes, noticias ambiguas
        - Miedo: Regulaciones adversas, hacks, crisis, ca√≠das abruptas
        
        CRITERIOS PARA NEWS_CATEGORY:
        - Regulaci√≥n: Leyes, normativas, decisiones gubernamentales
        - Tecnolog√≠a/Adopci√≥n: Avances t√©cnicos, nuevas integraciones
        - Mercado/Trading: Precios, an√°lisis t√©cnico, movimientos de mercado
        - Seguridad: Hacks, vulnerabilidades, protocolos de seguridad
        - Macroeconom√≠a: Inflaci√≥n, pol√≠tica monetaria, econom√≠a global
        
        Texto: "{text}"
        
        Responde √öNICAMENTE con el JSON:
        """
    
    def _parse_response(self, response_text: str, original_text: str) -> Dict[str, Any]:
        """Parsea y valida la respuesta de Gemini."""
        try:
            # Limpiar respuesta de bloques de c√≥digo markdown
            clean_text = response_text.strip()
            if clean_text.startswith('```json'):
                clean_text = clean_text.replace('```json\n', '').replace('\n```', '')
            elif clean_text.startswith('```'):
                lines = clean_text.split('\n')
                clean_text = '\n'.join(lines[1:-1])
            
            result = json.loads(clean_text)
            
            # Validar y normalizar los datos
            sentiment_score = float(result.get("sentiment_score", 0.0))
            sentiment_score = max(-1.0, min(1.0, sentiment_score))
            
            primary_emotion = result.get("primary_emotion", "Neutral")
            if primary_emotion not in VALID_EMOTIONS:
                logger.warning(f"Emoci√≥n inv√°lida '{primary_emotion}', usando 'Neutral'")
                primary_emotion = "Neutral"
            
            news_category = result.get("news_category", "Mercado/Trading")
            if news_category not in VALID_CATEGORIES:
                logger.warning(f"Categor√≠a inv√°lida '{news_category}', usando 'Mercado/Trading'")
                news_category = "Mercado/Trading"
            
            return {
                "sentiment_score": sentiment_score,
                "primary_emotion": primary_emotion,
                "news_category": news_category
            }
            
        except json.JSONDecodeError as e:
            logger.error(f"Error parseando JSON de Gemini: {e}. Respuesta: '{response_text[:200]}...'")
            return self._get_default_analysis()
    
    def _get_default_analysis(self) -> Dict[str, Any]:
        """Retorna an√°lisis por defecto cuando falla el procesamiento."""
        return {
            "sentiment_score": 0.0,
            "primary_emotion": "Neutral",
            "news_category": "Mercado/Trading"
        }


class RedditPostProcessor:
    """Procesador de posts de Reddit con filtros de calidad."""
    
    def __init__(self, analyzer: SentimentAnalyzer):
        """Inicializa el procesador con el analizador de sentimientos."""
        self.analyzer = analyzer
        self.processed_urls = load_processed_urls()  # Cargar URLs ya procesadas
        logger.info(f"üîÑ Iniciando con {len(self.processed_urls):,} URLs ya procesadas en memoria")
        self.stats = {
            'total_read': 0,
            'filtered_irrelevant_subreddit': 0,
            'filtered_low_quality': 0,
            'filtered_low_engagement': 0,
            'filtered_short_text': 0,
            'filtered_bad_link': 0,
            'filtered_duplicate_url': 0,  # Nueva m√©trica para duplicados
            'processed': 0,
            'errors': 0
        }
    
    def process_post(self, line: str) -> Optional[Dict[str, Any]]:
        """
        Procesa una l√≠nea del archivo de Reddit aplicando filtros.
        
        Args:
            line: L√≠nea JSON del archivo de Reddit
            
        Returns:
            Dict con datos del post procesado o None si se filtra
        """
        self.stats['total_read'] += 1
        
        try:
            post = json.loads(line.strip())
        except json.JSONDecodeError:
            self.stats['errors'] += 1
            return None
        
        # Aplicar pipeline de filtros
        if not self._filter_subreddit_relevance(post):
            self.stats['filtered_irrelevant_subreddit'] += 1
            return None
        
        if not self._filter_basic_quality(post):
            self.stats['filtered_low_quality'] += 1
            return None
        
        if not self._filter_minimum_engagement(post):
            self.stats['filtered_low_engagement'] += 1
            return None
        
        # Determinar tipo de post y texto a analizar
        text_to_analyze = self._extract_text_to_analyze(post)
        if not text_to_analyze:
            if post.get('is_self', False):
                self.stats['filtered_short_text'] += 1
            else:
                self.stats['filtered_bad_link'] += 1
            return None
        
        # Verificar si ya procesamos esta URL
        post_url = post.get('url', f"https://reddit.com{post.get('permalink', '')}")
        if post_url in self.processed_urls:
            self.stats['filtered_duplicate_url'] += 1
            return None
        
        # Analizar sentimiento
        try:
            analysis = self.analyzer.analyze_text_with_gemini(text_to_analyze)
            
            # Agregar URL al set de procesadas DESPU√âS de an√°lisis exitoso
            self.processed_urls.add(post_url)
            
            # Preparar datos para inserci√≥n en BD
            post_data = {
                'source': 'reddit_historical',
                'headline': text_to_analyze,
                'url': post_url,
                'published_at': str(datetime.fromtimestamp(post.get('created_utc', 0))),
                'sentiment_score': analysis['sentiment_score'],
                'primary_emotion': analysis['primary_emotion'],
                'news_category': analysis['news_category']
            }
            
            self.stats['processed'] += 1
            return post_data
            
        except Exception as e:
            logger.error(f"Error procesando post: {e}")
            self.stats['errors'] += 1
            return None
    
    def _filter_subreddit_relevance(self, post: Dict) -> bool:
        """Filtra por relevancia del subreddit."""
        subreddit = post.get('subreddit', '').strip()
        return subreddit in CRYPTO_SUBREDDITS
    
    def _filter_basic_quality(self, post: Dict) -> bool:
        """Filtra por calidad b√°sica del post."""
        selftext = post.get('selftext', '').strip()
        if selftext in ['[deleted]', '[removed]']:
            return False
        
        if post.get('stickied', False):
            return False
        
        return True
    
    def _filter_minimum_engagement(self, post: Dict) -> bool:
        """Filtra por engagement m√≠nimo."""
        score = post.get('score', 0)
        return score >= 10
    
    def _extract_text_to_analyze(self, post: Dict) -> Optional[str]:
        """Extrae el texto a analizar seg√∫n el tipo de post."""
        title = post.get('title', '').strip()
        is_self = post.get('is_self', False)
        
        if is_self:
            # Post de texto
            selftext = post.get('selftext', '').strip()
            if len(selftext) > 150:
                return f"{title} {selftext}"
            return None
        else:
            # Post de enlace
            url = post.get('url', '')
            if self._is_trusted_domain(url):
                return title
            return None
    
    def _is_trusted_domain(self, url: str) -> bool:
        """Verifica si el dominio est√° en la lista blanca."""
        try:
            domain = urlparse(url).netloc.lower()
            # Remover 'www.' si existe
            if domain.startswith('www.'):
                domain = domain[4:]
            return domain in TRUSTED_NEWS_DOMAINS
        except Exception:
            return False
    
    def get_stats(self) -> Dict[str, int]:
        """Retorna estad√≠sticas del procesamiento."""
        return self.stats.copy()


class DatabaseManager:
    """Manejador de la base de datos con inserci√≥n en lotes y fallback a SQLite."""
    
    def __init__(self):
        """Inicializa la conexi√≥n a la base de datos principal."""
        try:
            self.engine = create_engine(settings.DATABASE_URL)
            Base.metadata.create_all(bind=self.engine)
            Session = sessionmaker(bind=self.engine)
            self.session = Session()
            logger.info("Conexi√≥n a la base de datos principal establecida")
            
            # Inicializar base de datos SQLite de fallback
            self._init_fallback_db()
            
        except Exception as e:
            logger.error(f"Error conectando a la base de datos principal: {e}")
            raise
    
    def _init_fallback_db(self):
        """Inicializa la base de datos SQLite de fallback."""
        try:
            self.fallback_engine = create_engine(f"sqlite:///{FALLBACK_DB_PATH}")
            Base.metadata.create_all(bind=self.fallback_engine)
            FallbackSession = sessionmaker(bind=self.fallback_engine)
            self.fallback_session = FallbackSession()
            logger.info(f"Base de datos SQLite de fallback inicializada: {FALLBACK_DB_PATH}")
        except Exception as e:
            logger.error(f"Error inicializando base de datos de fallback: {e}")
            self.fallback_session = None
    
    def _insert_to_fallback(self, batch_data: List[Dict[str, Any]]) -> bool:
        """
        Inserta datos en la base de datos SQLite de fallback.
        
        Args:
            batch_data: Lista de diccionarios con datos de posts
            
        Returns:
            True si la inserci√≥n fue exitosa
        """
        if not self.fallback_session:
            logger.error("Sesi√≥n de fallback no disponible")
            return False
            
        try:
            # Usar raw SQL con INSERT OR IGNORE para SQLite
            from sqlalchemy import text
            
            insert_sql = """
            INSERT INTO noticias (source, headline, url, published_at, sentiment_score, primary_emotion, news_category)
            VALUES (:source, :headline, :url, :published_at, :sentiment_score, :primary_emotion, :news_category)
            ON CONFLICT(url) DO UPDATE SET
                headline = CASE 
                    WHEN LENGTH(excluded.headline) > LENGTH(noticias.headline) THEN excluded.headline
                    ELSE noticias.headline
                END,
                sentiment_score = excluded.sentiment_score,
                primary_emotion = excluded.primary_emotion,
                news_category = excluded.news_category
            """
            
            self.fallback_session.execute(text(insert_sql), batch_data)
            self.fallback_session.commit()
            logger.warning(f"[FALLBACK] Lote de {len(batch_data)} registros guardado en SQLite local (duplicados actualizados si son mejores)")
            return True
        except Exception as e:
            logger.error(f"Error insertando en base de datos de fallback: {e}")
            self.fallback_session.rollback()
            return False
    
    def insert_batch(self, batch_data: List[Dict[str, Any]]) -> bool:
        """
        Inserta un lote de datos en la base de datos con reintentos y fallback.
        
        Args:
            batch_data: Lista de diccionarios con datos de posts
            
        Returns:
            True si la inserci√≥n fue exitosa (en BD principal o fallback)
        """
        # Intentar inserci√≥n en base de datos principal con manejo de duplicados
        for attempt in range(DB_INSERT_RETRIES):
            try:
                # Usar raw SQL con ON CONFLICT para PostgreSQL
                from sqlalchemy import text
                
                insert_sql = """
                INSERT INTO noticias (source, headline, url, published_at, sentiment_score, primary_emotion, news_category)
                VALUES (:source, :headline, :url, :published_at, :sentiment_score, :primary_emotion, :news_category)
                ON CONFLICT (url) DO UPDATE SET
                    headline = CASE 
                        WHEN LENGTH(EXCLUDED.headline) > LENGTH(noticias.headline) THEN EXCLUDED.headline
                        ELSE noticias.headline
                    END,
                    sentiment_score = EXCLUDED.sentiment_score,
                    primary_emotion = EXCLUDED.primary_emotion,
                    news_category = EXCLUDED.news_category
                """
                
                self.session.execute(text(insert_sql), batch_data)
                self.session.commit()
                logger.info(f"Lote de {len(batch_data)} registros insertado exitosamente en BD principal (duplicados actualizados si son mejores)")
                return True
                
            except Exception as e:
                logger.warning(f"Error en intento {attempt + 1}/{DB_INSERT_RETRIES} de inserci√≥n en BD principal: {e}")
                self.session.rollback()
                
                # Si no es el √∫ltimo intento, esperar antes de reintentar
                if attempt < DB_INSERT_RETRIES - 1:
                    wait_time = 2 ** attempt  # Exponential backoff: 1s, 2s, 4s
                    logger.info(f"Esperando {wait_time}s antes de reintentar...")
                    time.sleep(wait_time)
        
        # Si llegamos aqu√≠, todos los intentos fallaron
        logger.error(f"[ERROR] Fall√≥ la inserci√≥n en BD principal despu√©s de {DB_INSERT_RETRIES} intentos")
        
        # Intentar fallback a SQLite
        logger.warning("[RETRY] Intentando guardar en base de datos de fallback...")
        if self._insert_to_fallback(batch_data):
            return True
        else:
            logger.error("[ERROR] Tambi√©n fall√≥ la inserci√≥n en base de datos de fallback")
            return False
    
    def close(self):
        """Cierra las conexiones a las bases de datos."""
        if self.session:
            self.session.close()
            logger.info("Conexi√≥n a la base de datos principal cerrada")
            
        if hasattr(self, 'fallback_session') and self.fallback_session:
            self.fallback_session.close()
            logger.info("Conexi√≥n a la base de datos de fallback cerrada")


def get_file_size(filepath: str) -> int:
    """Obtiene el tama√±o del archivo en bytes."""
    try:
        return os.path.getsize(filepath)
    except OSError:
        return 0

def count_lines_in_zst(filepath: str) -> int:
    """
    FUNCI√ìN DESHABILITADA: Contaba l√≠neas pero causaba MemoryError en archivos grandes.
    Se mantiene comentada para referencia hist√≥rica.
    """
    # Esta funci√≥n se deshabilit√≥ porque causa problemas de memoria en archivos grandes
    # Al leer todo el archivo para contar l√≠neas antes del procesamiento principal
    logger.warning("count_lines_in_zst est√° deshabilitada para evitar MemoryError")
    # Estimaci√≥n simple basada en tama√±o del archivo
    file_size = get_file_size(filepath)
    return file_size // 1024  # Estimaci√≥n: ~1KB por l√≠nea


def process_reddit_file(filepath: str) -> Dict[str, Any]:
    """
    Procesa el archivo de Reddit hist√≥rico completo.
    
    Args:
        filepath: Ruta al archivo .zst
        
    Returns:
        Dict con estad√≠sticas del procesamiento
    """
    logger.info(f"Iniciando procesamiento de archivo: {filepath}")
    
    # Obtener informaci√≥n b√°sica del archivo
    file_size_bytes = get_file_size(filepath)
    file_size_gb = file_size_bytes / (1024**3)
    
    logger.info(f"üìä AN√ÅLISIS DEL ARCHIVO:")
    logger.info(f"   Tama√±o: {file_size_gb:.2f} GB")
    logger.info(f"   Estrategia: Comando externo zstd con streaming")
    logger.info(f"   Procesamiento: L√≠nea por l√≠nea sin pre-carga")
    logger.info(f"   Par√°metros: --long=31 (window hasta 2GB), --memory=2048MB")
    
    # Variables para procesamiento en lotes
    batch_data = []
    batch_insertions = 0
    start_time = time.time()
    db_manager = None
    
    # Cargar checkpoint si existe
    start_line = load_checkpoint()
    if start_line > 0:
        logger.info(f"üîÑ REANUDANDO procesamiento desde l√≠nea {start_line:,}")
    else:
        logger.info("üöÄ INICIANDO procesamiento desde el principio")
    
    try:
        # Inicializar componentes paso a paso con mejor error handling
        logger.info("Inicializando analizador de sentimientos...")
        analyzer = SentimentAnalyzer()
        
        logger.info("Inicializando procesador de posts...")
        processor = RedditPostProcessor(analyzer)
        
        logger.info("Inicializando gestor de base de datos...")
        db_manager = DatabaseManager()
        
        # Usar barra de progreso sin total para evitar problemas de memoria
        logger.info("Iniciando procesamiento sin pre-conteo de l√≠neas (m√°s eficiente en memoria)")
        total_lines = None  # Sin total conocido
        
        # Abrir y procesar archivo usando comando externo zstd (m√°s eficiente para archivos grandes)
        logger.info("Abriendo archivo para procesamiento usando comando externo zstd...")
        logger.info("Verificando disponibilidad del comando zstd...")
        
        # Verificar que zstd est√© disponible
        if not check_zstd_availability():
            logger.error("‚ùå ERROR: Comando 'zstd' no disponible. Terminando script.")
            sys.exit(1)
        
        logger.info("Iniciando descompresi√≥n con comando externo zstd...")
        logger.info("Usando par√°metros para archivos con window size grande...")
        # Usamos subprocess para descomprimir streaming con zstd command line
        # --long=31: Permite window sizes hasta 2GB (2^31 bytes)
        # --memory=2048MB: L√≠mite de memoria para decodificaci√≥n
        process = subprocess.Popen(
            ['zstd', '-d', '-c', '--memory=4096MB', filepath],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            encoding='utf-8',
            bufsize=1  # Line buffered
        )
        
        logger.info("Iniciando procesamiento l√≠nea por l√≠nea desde stdout de zstd...")
        # Procesar con barra de progreso (sin total conocido para eficiencia de memoria)
        with tqdm(desc="Procesando posts", unit="posts", total=None) as pbar:
            line_count = 0
            
            try:
                # Verificar que el proceso est√© funcionando
                if process.stdout is None:
                    raise RuntimeError("No se pudo abrir stdout del proceso zstd")
                
                # Ahora podemos iterar l√≠nea por l√≠nea desde el proceso zstd
                for line in process.stdout:
                    line_count += 1
                    
                    # Saltar l√≠neas ya procesadas (checkpoint)
                    if line_count <= start_line:
                        pbar.update(1)
                        continue
                    
                    try:
                        if not line.strip():
                            pbar.update(1)
                            continue
                        
                        # Procesar post
                        post_data = processor.process_post(line)
                        
                        if post_data:
                            batch_data.append(post_data)
                        
                        # Insertar lote cuando se alcance el tama√±o objetivo
                        if len(batch_data) >= BATCH_SIZE:
                            success = db_manager.insert_batch(batch_data)
                            if success:
                                batch_insertions += 1
                            else:
                                logger.error("Fall√≥ inserci√≥n de lote completamente")
                            batch_data.clear()
                            
                            # Guardar checkpoint cada lote procesado
                            save_checkpoint(line_count)
                            
                            # Guardar URLs procesadas cada 10 lotes (5000 registros)
                            if batch_insertions % 10 == 0:
                                save_processed_urls(processor.processed_urls)
                        
                        pbar.update(1)
                        
                        # Actualizar descripci√≥n de progreso cada 1000 posts
                        if pbar.n % 1000 == 0:
                            stats = processor.get_stats()
                            # Monitoreo de memoria cada 10,000 posts
                            if pbar.n % 10000 == 0:
                                memory_percent = psutil.virtual_memory().percent
                                logger.info(f"Checkpoint {pbar.n:,}: Uso de memoria: {memory_percent:.1f}%")
                            
                            pbar.set_description(
                                f"Procesando posts (procesados: {stats['processed']}, "
                                f"errores: {stats['errors']})"
                            )
                    
                    except json.JSONDecodeError:
                        # Ignoramos l√≠neas que no son JSON v√°lido, es com√∫n en estos archivos
                        pbar.update(1)
                        continue
                    except Exception as e:
                        logger.error(f"Error procesando l√≠nea {line_count}: {e}")
                        pbar.update(1)
                        continue
                        
            except Exception as e:
                logger.error(f"Error en el procesamiento con comando zstd: {e}")
                raise
            finally:
                # Asegurar que el proceso se termine correctamente
                if process.stdout:
                    process.stdout.close()
                return_code = process.wait()
                if return_code != 0:
                    stderr_output = ""
                    if process.stderr:
                        stderr_output = process.stderr.read()
                    logger.error(f"Error en comando zstd (c√≥digo {return_code}): {stderr_output}")
                    raise subprocess.CalledProcessError(return_code, 'zstd', stderr_output)
        
        # Insertar lote final si tiene datos
        if batch_data:
            success = db_manager.insert_batch(batch_data)
            if success:
                batch_insertions += 1
            else:
                logger.error("Fall√≥ inserci√≥n del lote final")
        
        # Guardar URLs procesadas antes de limpiar
        save_processed_urls(processor.processed_urls)
    
    except Exception as e:
        logger.error(f"Error en el procesamiento: {e}")
        if db_manager:
            db_manager.close()
        raise
    
    finally:
        if db_manager:
            db_manager.close()
    
    # Calcular estad√≠sticas finales
    end_time = time.time()
    processing_time = end_time - start_time
    stats = processor.get_stats()
    
    # Limpiar checkpoint al completar exitosamente
    clear_checkpoint()
    clear_processed_urls()
    
    logger.info("=" * 60)
    logger.info("PROCESAMIENTO COMPLETADO")
    logger.info("=" * 60)
    logger.info(f"Tiempo total: {processing_time:.2f} segundos")
    logger.info(f"Posts le√≠dos: {stats['total_read']:,}")
    logger.info(f"Posts procesados: {stats['processed']:,}")
    logger.info(f"Lotes insertados: {batch_insertions}")
    logger.info(f"Errores: {stats['errors']:,}")
    logger.info("")
    
    # Verificar si se us√≥ fallback
    if os.path.exists(FALLBACK_DB_PATH):
        file_size = os.path.getsize(FALLBACK_DB_PATH)
        if file_size > 0:
            logger.warning("[WARNING] ATENCI√ìN: Se utiliz√≥ base de datos de fallback SQLite")
            logger.warning(f"   Archivo: {FALLBACK_DB_PATH}")
            logger.warning(f"   Tama√±o: {file_size / (1024*1024):.2f} MB")
            logger.warning("   Recuerda migrar estos datos a la BD principal cuando sea posible")
        else:
            logger.info("[OK] No se requiri√≥ usar la base de datos de fallback")
    else:
        logger.info("[OK] No se requiri√≥ usar la base de datos de fallback")
    
    logger.info("")
    logger.info("FILTROS APLICADOS:")
    logger.info(f"  - Subreddit irrelevante: {stats['filtered_irrelevant_subreddit']:,}")
    logger.info(f"  - Baja calidad: {stats['filtered_low_quality']:,}")
    logger.info(f"  - Poco engagement: {stats['filtered_low_engagement']:,}")
    logger.info(f"  - Texto muy corto: {stats['filtered_short_text']:,}")
    logger.info(f"  - Enlace no confiable: {stats['filtered_bad_link']:,}")
    logger.info(f"  - URL duplicada: {stats['filtered_duplicate_url']:,}")
    logger.info("=" * 60)
    
    return {
        'success': True,
        'processing_time': processing_time,
        'posts_processed': stats['processed'],
        'batch_insertions': batch_insertions,
        'stats': stats
    }


def save_checkpoint(line_number: int) -> None:
    """Guarda el progreso actual en un archivo de checkpoint."""
    try:
        with open(CHECKPOINT_FILE, 'w') as f:
            f.write(str(line_number))
        logger.info(f"Checkpoint guardado en l√≠nea {line_number:,}")
    except Exception as e:
        logger.warning(f"No se pudo guardar checkpoint: {e}")

def load_checkpoint() -> int:
    """Carga el progreso desde el archivo de checkpoint."""
    try:
        if os.path.exists(CHECKPOINT_FILE):
            with open(CHECKPOINT_FILE, 'r') as f:
                line_number = int(f.read().strip())
            logger.info(f"Checkpoint encontrado: reanudando desde l√≠nea {line_number:,}")
            return line_number
        return 0
    except Exception as e:
        logger.warning(f"No se pudo cargar checkpoint: {e}")
        return 0

def clear_checkpoint() -> None:
    """Elimina el archivo de checkpoint al completar el procesamiento."""
    try:
        if os.path.exists(CHECKPOINT_FILE):
            os.remove(CHECKPOINT_FILE)
            logger.info("Checkpoint eliminado - procesamiento completado")
    except Exception as e:
        logger.warning(f"No se pudo eliminar checkpoint: {e}")


def save_processed_urls(processed_urls: set) -> None:
    """Guarda el set de URLs procesadas en un archivo."""
    try:
        with open(PROCESSED_URLS_FILE, 'w', encoding='utf-8') as f:
            for url in processed_urls:
                f.write(f"{url}\n")
        logger.info(f"URLs procesadas guardadas: {len(processed_urls):,}")
    except Exception as e:
        logger.warning(f"No se pudieron guardar URLs procesadas: {e}")

def load_processed_urls() -> set:
    """Carga el set de URLs procesadas desde un archivo."""
    processed_urls = set()
    try:
        if os.path.exists(PROCESSED_URLS_FILE):
            with open(PROCESSED_URLS_FILE, 'r', encoding='utf-8') as f:
                processed_urls = {line.strip() for line in f if line.strip()}
            logger.info(f"URLs procesadas cargadas: {len(processed_urls):,}")
        return processed_urls
    except Exception as e:
        logger.warning(f"No se pudieron cargar URLs procesadas: {e}")
        return set()

def clear_processed_urls() -> None:
    """Elimina el archivo de URLs procesadas al completar el procesamiento."""
    try:
        if os.path.exists(PROCESSED_URLS_FILE):
            os.remove(PROCESSED_URLS_FILE)
            logger.info("Archivo de URLs procesadas eliminado - procesamiento completado")
    except Exception as e:
        logger.warning(f"No se pudo eliminar archivo de URLs procesadas: {e}")


def main():
    """Funci√≥n principal del script."""
    parser = argparse.ArgumentParser(
        description='Procesa archivos masivos de Reddit hist√≥ricos con an√°lisis de sentimiento'
    )
    parser.add_argument(
        'file_path',
        help='Ruta al archivo .zst de Reddit a procesar'
    )
    
    args = parser.parse_args()
    
    # Validar que el archivo existe
    if not os.path.exists(args.file_path):
        logger.error(f"El archivo no existe: {args.file_path}")
        sys.exit(1)
    
    # Validar extensi√≥n del archivo
    if not args.file_path.endswith('.zst'):
        logger.error("El archivo debe tener extensi√≥n .zst")
        sys.exit(1)
    
    # Validar configuraci√≥n
    if not settings.GOOGLE_API_KEY:
        logger.error("GOOGLE_API_KEY no configurada en el archivo .env")
        sys.exit(1)
    
    if not settings.DATABASE_URL:
        logger.error("DATABASE_URL no configurada en el archivo .env")
        sys.exit(1)
    
    # CHEQUEOS PREVIOS AL INICIO (Pre-flight Checks)
    logger.info("=" * 60)
    logger.info("REALIZANDO CHEQUEOS PREVIOS")
    logger.info("=" * 60)
    
    # Chequeo 1: Verificar conexi√≥n a la base de datos
    if not check_database_connection():
        logger.error("[ERROR] Fallo en chequeo de base de datos. Terminando script.")
        sys.exit(1)
    
    # Chequeo 2: Verificar API de Gemini
    if not check_gemini_api():
        logger.error("[ERROR] Fallo en chequeo de API de Gemini. Terminando script.")
        sys.exit(1)
    
    # Chequeo 3: Verificar disponibilidad de comando zstd
    if not check_zstd_availability():
        logger.error("[ERROR] Fallo en chequeo de comando zstd. Terminando script.")
        sys.exit(1)
    
    logger.info("[OK] Todos los chequeos previos completados exitosamente")
    logger.info("=" * 60)
    
    try:
        result = process_reddit_file(args.file_path)
        if result['success']:
            logger.info("[OK] Procesamiento completado exitosamente")
            sys.exit(0)
        else:
            logger.error("[ERROR] El procesamiento fall√≥")
            sys.exit(1)
    except Exception as e:
        logger.error(f"Error fatal en el procesamiento: {e}")
        logger.error(f"Tipo de error: {type(e).__name__}")
        import traceback
        logger.error(f"Traceback completo:\n{traceback.format_exc()}")
        sys.exit(1)


if __name__ == "__main__":
    main()